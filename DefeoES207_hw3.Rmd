---
title: "DefeoES207_hw3"
author: "Shelby Defeo"
date: "3/2/2022"
output: html_document
---
1. Import the data and compare the chloride concentrations from the two rock types. In particular, generate and compare the following graphs for chloride concentrations of the two rock types: a) histogram, b) boxplot, c) Q-Q plot. Describe the similarities and differences in chloride between these two rock types. What characteristics are evident in each graph?
The histograms of both rock types are not normally distributed and are skewed to the right with most of the data being on the left and having a longer tail on the right. Quartz monzonite has higher concentrations of chloride than Granodiorite. With the boxplots, it appears that both types of rock have a datapoint that is higher than the others. The center of the data for both rock types is shifted towards the lower end of the graph. For Granodiorite, the high value is about 10 while for Quartz monzonite it is about 3.The diversion from normal for Granodiorite is easier to see in the Q-Q plot than for Quartz monzonite since it dips below and above the normal line more strongly. This means that it has larger tails and has less data in the center of the distribution.

```{r}
#1. Import the data and compare the chloride concentrations from the two rock types. In particular, generate and compare the following graphs for chloride concentrations of the two rock types: a) histogram, b) boxplot, c) Q-Q plot. Describe the similarities and differences in chloride between these two rock types. What characteristics are evident in each graph?
library(tidyverse)
#Checking working directory to be able to input file
getwd()
CC <- read_csv("ChlorideConcentrations.csv")

#Separate columns as unique variables so they can be analyzed individually
G <- CC$Granodiorite
Q <- CC$`Quartz monzonite`

#create hisotgrams for G and Q
hist(G,prob=TRUE,xlim=c(0,12))
hist(Q,prob=TRUE,xlim=c(0,4))

#Create boxplots for G and Q
boxplot(G)
boxplot(Q)

#Create Q-Q plots for G and Q
qqnorm(G)
qqline(G)
qqnorm(Q)
qqline(Q)
```

2. Using the chloride concentrations (from Question 1), compute 95% Confidence Interval estimates for the median of the granodiorite data.
The 95% CI estimates for the median of granodiorite is 0.5145246 and 0.7854754.

```{r}
#2. Using the chloride concentrations (from Question 1), compute 95% Confidence Interval estimates for the median of the granodiorite data.
Gmedian <- median(G)
Gmedian 

Gn <- length(G)
Gn

#insert code for median and standard error
G_hat <- median(G)
GSE <- sd(G)/(Gn)^1/2
G_hat
GSE

#insert code for alpha value for 95% confidence
Galpha=(1 - 0.95)

Galpha

#assign critical t-value to variable t (use t because we have a small sample size of 18 [which is less than 30])

Gt <- 1.740
Gt

#insert code for margin of error and confidence interval 
GME <- Gt * GSE
GME
GLB <- G_hat - GME
GUB <- G_hat + GME
GLB 
GUB
print("Our 95% CI is: (0.5145246, 0.7854754)")
```
Scenario:You have been collecting data on well yields from across the state and storing your measurements in a dataset labeled VAwells.csv. You encounter a new well and find that its yield is 0.85 gallons per minute per foot.
3. Is the yield of the new well likely to belong to the same distribution as the data in VAwells.csv or does it represent something larger? Use a 95% Prediction Interval to inform your answer.
The new yield is likely to belong to the same distribution as the data in VAWells because it falls within our 95% predicition interval.(0.3123<0.85<0.905685)

```{r}
#3. Is the yield of the new well likely to belong to the same distribution as the data in VAwells.csv or does it represent something larger? Use a 95% Prediction Interval to inform your answer.

#importing and organizing the data
Wells <- read_csv("VAWells.csv")
wy <- Wells$yields

#finding the n of our data
wy_length <- length(wy)
wy_length

# code for estimating 95% PI for the mean well yield
wy_mean <- mean(wy)
wy_mean
wy_t <- qt(0.975,11)
wy_t
wy_stdev <- sd(wy)
wy_stdev
wy_PI_low <- wy_mean - (wy_t * wy_stdev * (sqrt(1 + (1/12))))
wy_PI_low
wy_PI_high <- wy_mean + (wy_t * wy_stdev * (sqrt(1 + (1/12))))
wy_PI_high
print("Our 95% PI is: (0.3123, 0.905685)")
```



4. Import Conecuh.csv and construct the 95% Confidence and Prediction Intervals for the mean and median annual stream flows.
The 95% CI for the mean is 556.6 to 808.9. The 95% CI for the median is 454.9 and 707.14. The 95% PI for the mean is 104.72 to 1260.78. The 95% PI for the median is 2.97 to 1159.03. As expected, the prediction intervals are wider than the confidence intervals.
```{r}
#4. Import Conecuh.csv and construct the 95% Confidence and Prediction Intervals for the mean and median annual stream flows.
#Import Excel file
library(tidyverse)
Con <- read_csv("Conecuh.csv")

#separate out data
flow <- Con$Flowcfs
flow

#finding the n of our data
f_n <-length(flow)
f_n

#finding t values (because we have n < 30)
flow_t <- qt(0.975,19)
flow_t




#95% CI for the mean
CImean_hat <- mean(flow)  #find the mean of the data
CI_SE <- sd(flow)/sqrt(f_n) #find the standard error of the data
CImean_hat #print mean of the data
CI_SE #print SE of the data

CI_ME <- flow_t*CI_SE #find margin of error of the data
CI_ME  #print margin of error

CImean_LB <- CImean_hat - CI_ME #Find lower bound
CImean_UB <- CImean_hat + CI_ME #find upper bound
CImean_LB  #print lower bound
CImean_UB  #print upper bound




#95% CI for median
CImedian_hat <- median(flow) #find median of the data
CImedian_hat

CImedian_LB <- CImedian_hat - CI_ME #Find lower bound
CImedian_UB <- CImedian_hat + CI_ME #find upper bound
CImedian_LB  #print lower bound
CImedian_UB  #print upper bound






#95% PI for mean
flow_stdev <- sd(flow) #Standard deviation of flow
flow_stdev

flow_mean <- mean(flow) #mean of flow
flow_mean

flow_PImean_low <- flow_mean - (flow_t * flow_stdev * (sqrt(1 + (1/20)))) #lower bound of mean PI
flow_PImean_low
flow_PImean_high <- flow_mean + (flow_t * flow_stdev * (sqrt(1 + (1/20)))) #upper bound of mean PI
flow_PImean_high




#95% PI for median
flow_median <- median(flow) #median of flow
flow_median

flow_PImedian_low <- flow_median - (flow_t * flow_stdev * (sqrt(1 + (1/20)))) #lower bound of median PI
flow_PImedian_low
flow_PImedian_high <- flow_median + (flow_t * flow_stdev * (sqrt(1 + (1/20)))) #upper bound of median PI
flow_PImedian_high
```


5. Using Conecuh.csv, apply a bootstrap approach to construct the 95% Confidence Interval for the mean annual streamflows. How does your answer change if you use 1000 bootstrap replicates vs. 10000 bootstrap replicates? How does your bootstrap Confidence Interval compare to your estimate in Question 4?
The 1000 bootstrap replicate versus the 10000 bootstrap replicates doesn't change a whole lot. Both of the lower bounds are around 570 and both of the upper bounds are around 800.The confidence intervals calculated in this question are slightly smaller than the ones I calculated in the previous question.
```{r}
#5. Using Conecuh.csv, apply a bootstrap approach to construct the 95% Confidence Interval for the mean annual streamflows. How does your answer change if you use 1000 bootstrap replicates vs. 10000 bootstrap replicates? How does your bootstrap Confidence Interval compare to your estimate in Question 4?

#initial sample of size 20 from population data
samp_flow <- sample(flow,20)

#bootstrap for 1000 replicates
bstrap_low <- c() #initialize vector to store results of for loop
for (i in 1:1000){
  bstrap_low <- c(bstrap_low, mean(sample(samp_flow, 20, replace = T)))
} #run loop for bootstrap to randomly select from the 20 available data points 1000 times

lower_bound_low <- quantile(bstrap_low, 0.025) #lower bound of 95% CI for bootstrap
upper_bound_low <- quantile(bstrap_low, 0.975) #upper bound of 95% CI for bootstrap

lower_bound_low
upper_bound_low


#bootstrap for 10000 replicates
bstrap_high <- c() #initialize vector to store results of for loop
for (i in 1:10000){
  bstrap_high <- c(bstrap_high, mean(sample(samp_flow, 20, replace = T)))
} #run loop for bootstrap to randomly select from the 20 available data points 10000 times

lower_bound_high <- quantile(bstrap_high, 0.025) #lower bound of 95% CI for bootstrap
upper_bound_high <- quantile(bstrap_high, 0.975) #upper bound of 95% CI for bootstrap

lower_bound_high
upper_bound_high
```

CQ1. Write a function that calculates the mean, median, standard deviation, interquartile range, and QQ-Plot. Apply that function to the TP concentration data. In ~300 words or less discuss the differences between these measures and what conclusions you can draw about the data.
Mean and median are both measures of center, but the mean is the sum of all observations divided by the number of values and the median is the middle number when the data is ordered sequentially. The mean is more susceptible to the influence of outliers so when you have skewed data, it can be better to use the median for a measure of center.Standard deviation and IQR are both measures of spread and the variability of a sample.IQR is the range of the middle half of the data while standard deviation is the average distance from the mean. Standard deviation is more susceptible to outliers than IQR because it depends on individual values.
The mean is 59.8, the median is 40.2, the standard deviation is 57.5, and the IQR is 49.4. The mean is larger than the median so we can say that the distribution is positively skewed. We also see the positive skew of the data in the QQ-Plot because it diverges above where the normal line.There is a fair bit of spread in our data set so we can't be as confident in predictions we make on the sample being applicable to the population.

```{r}
#CQ1. Write a function that calculates the mean, median, standard deviation, interquartile range, and QQ-Plot. Apply that function to the TP concentration data. In ~300 words or less discuss the differences between these measures and what conclusions you can draw about the data.
library(tidyverse)
Lake <- read_csv("LakeErie2.csv") #import data

TP <- Lake$TP  #separate out TP



CQ1 <- function(TP) {
  mean_TP <- mean(TP, na.rm = TRUE)
  print(mean_TP)
  median_TP <- median(TP, na.rm = TRUE)
  print(median_TP)
  stdev_TP <- sd(TP, na.rm = TRUE)
  print(stdev_TP)
  IQR_TP <- IQR(TP, na.rm = TRUE)
  print(IQR_TP)
  qqnorm(TP)
  qqline(TP)
}

CQ1(TP)

```


CQ2. Using this information and dataset, what design discharge should be established for this intake in order to ensure that there is only a 5% probability that the true 0.1 frequency annual minimum flow is below the design discharge?
The data is not normally distributed and has a positive skew as seen with the mean being larger than the median and the deviation from the normal line in the qq plot. Therefore we need to use nonparametric tests to evaluate this. We should establish the design to be 20.3 minimum flow so that the intake is mostly properly submerged.

```{r}
#CQ2. Using this information and dataset, what design discharge should be established for this intake in order to ensure that there is only a 5% probability that the true 0.1 frequency annual minimum flow is below the design discharge?
library(tidyverse)
discharge <- read_csv("PotomacOneDayLow.csv") #import data
min_dis <- discharge$Q
min_dis

#Evaluating normality to determine suitability of tests
qqnorm(min_dis)
qqline(min_dis)

mean(min_dis)
median(min_dis)

length(min_dis)
n <- 84

#evaluate likelihood of scenarios
min_disHatP <- quantile(min_dis, probs = 0.1, type = 6)
min_disHatP

p = 0.1 
z95 <- qnorm(0.95)
RU <- n * p + z95 * sqrt(n * p * (1 - p)) + 0.5 
RU

xUpper <- quantile(min_dis, probs = RU/n, type = 6) 
xUpper
```

